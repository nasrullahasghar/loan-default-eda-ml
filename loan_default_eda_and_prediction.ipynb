{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3106815,"sourceType":"datasetVersion","datasetId":1897041}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirzanasrullah/loan-default-eda-and-pridiction?scriptVersionId=277287657\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:17.583873Z","iopub.execute_input":"2025-11-13T15:03:17.584142Z","iopub.status.idle":"2025-11-13T15:03:17.919792Z","shell.execute_reply.started":"2025-11-13T15:03:17.584118Z","shell.execute_reply":"2025-11-13T15:03:17.919175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **1. Introduction**","metadata":{}},{"cell_type":"markdown","source":"## **About the Author ðŸ‘¤**\nNasrullah Asghar\n\n[LinkedIn](https://www.linkedin.com/in/inasrullah-asghar)","metadata":{}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/Masterx-AI/Project_Loan_Default_Risk_Expectancy_/main/loan.jpg)","metadata":{}},{"cell_type":"markdown","source":"## **About the Data**","metadata":{}},{"cell_type":"markdown","source":"### ðŸ’° Once Upon a Loan â€” Meet the Data Behind Defaults\n\nEvery great story starts with an interesting cast â€” and ours begins with *borrowers*, *lenders*, and a mysterious column called **'Status'**.  \n\nIn the world of finance, not everyone keeps their promise to repay. Some do, some donâ€™t â€” and our mission is to find out **why**.  \n\nBefore we jump into detective mode ðŸ•µï¸â€â™‚ï¸, letâ€™s first *meet our dataset* and see what kind of clues it holds.\n","metadata":{}},{"cell_type":"markdown","source":"### Meta Data","metadata":{}},{"cell_type":"markdown","source":"**`Description:`**\\\nBanks earn a major revenue from lending loans. But it is often associated with risk. The borrower's may default on the loan. To mitigate this issue, the banks have decided to use Machine Learning to overcome this issue. They have collected past data on the loan borrowers & would like you to develop a strong ML Model to classify if any new borrower is likely to default or not.\n\nThe dataset is enormous & consists of multiple deteministic factors like borrowe's income, gender, loan pupose etc. The dataset is subject to strong multicollinearity & empty values. Can you overcome these factors & build a strong classifier to predict defaulters?\n\n**`Acknowledgements:`**\\\nThis dataset has been referred from [Kaggle](https://www.kaggle.com/datasets/yasserh/loan-default-dataset/data).\n\n**`Objective:`**\n- Understand the Dataset & cleanup (if required).\n- Build classification model to predict weather the loan borrower will default or not.\n- Also fine-tune the hyperparameters & compare the evaluation metrics of vaious classification algorithms.","metadata":{}},{"cell_type":"markdown","source":"### Import Pyhton Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:17.920958Z","iopub.execute_input":"2025-11-13T15:03:17.921236Z","iopub.status.idle":"2025-11-13T15:03:18.499564Z","shell.execute_reply.started":"2025-11-13T15:03:17.92122Z","shell.execute_reply":"2025-11-13T15:03:18.498737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load the Data Set","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/loan-default-dataset/Loan_Default.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:18.500511Z","iopub.execute_input":"2025-11-13T15:03:18.500775Z","iopub.status.idle":"2025-11-13T15:03:19.156018Z","shell.execute_reply.started":"2025-11-13T15:03:18.500758Z","shell.execute_reply":"2025-11-13T15:03:19.155056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### First Five Rows of Data","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.158145Z","iopub.execute_input":"2025-11-13T15:03:19.158374Z","iopub.status.idle":"2025-11-13T15:03:19.178939Z","shell.execute_reply.started":"2025-11-13T15:03:19.158357Z","shell.execute_reply":"2025-11-13T15:03:19.178042Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“˜ Dataset Dictionary\n\n| Column Name | Explanation |\n|--------------|-------------|\n| `ID` | Unique identifier for each loan record. |\n| `year` | The year when the loan application was processed (2019 for all records). |\n| `loan_limit` | Indicates whether the loan amount exceeded conforming loan limits (e.g., 'cf', 'ncf'). |\n| `Gender` | Gender of the primary applicant (Male/Female). |\n| `approv_in_adv` | Specifies whether the loan was approved in advance (â€˜pre-approvedâ€™ or â€˜not pre-approvedâ€™). |\n| `loan_type` | Type of loan applied for (e.g., Conventional, FHA, VA). |\n| `loan_purpose` | The reason for the loan (e.g., Home purchase, Refinancing, Improvement). |\n| `Credit_Worthiness` | Indicates applicantâ€™s credit rating (e.g., Good, Poor). |\n| `open_credit` | Describes whether the applicant has open lines of credit. |\n| `business_or_commercial` | States if the loan is for business/commercial purposes or personal use. |\n| `loan_amount` | The total amount of money requested for the loan. |\n| `rate_of_interest` | Interest rate charged on the loan. |\n| `Interest_rate_spread` | Difference between the loanâ€™s interest rate and the average market rate. |\n| `Upfront_charges` | Fees or charges paid upfront during loan origination. |\n| `term` | Duration (in months) for which the loan is granted (e.g., 360 months = 30 years). |\n| `Neg_ammortization` | Indicates if the loan allows negative amortization (interest added to the principal). |\n| `interest_only` | Whether the loan has an interest-only payment period. |\n| `lump_sum_payment` | Specifies if the borrower can make lump-sum payments. |\n| `property_value` | Appraised market value of the property securing the loan. |\n| `construction_type` | Type of construction (e.g., site-built, manufactured). |\n| `occupancy_type` | Specifies if the property will be owner-occupied, rented, or vacant. |\n| `Secured_by` | What secures the loan (e.g., Property, Land). |\n| `total_units` | Number of housing units covered by the property (e.g., 1, 2, 3, or 4 units). |\n| `income` | Applicantâ€™s annual income. |\n| `credit_type` | Type of credit instrument used (e.g., Conventional, Revolving). |\n| `Credit_Score` | Applicantâ€™s credit score (typically ranges between 500â€“900). |\n| `co-applicant_credit_type` | Type of credit for co-applicant, if any. |\n| `age` | Age group or bracket of the applicant. |\n| `submission_of_application` | Indicates whether the loan application was submitted directly or via an agent. |\n| `LTV` | Loan-to-Value ratio â€” shows the loan amount as a percentage of property value. |\n| `Region` | Geographic region where the loan was issued. |\n| `Security_Type` | Indicates type of security backing the loan (e.g., secured or unsecured). |\n| `Status` | **Target variable** â€” loan status: `1` = defaulted, `0` = non-defaulted. |\n| `dtir1` | Debt-to-Income Ratio â€” measures how much of the applicantâ€™s income goes toward debt payments. |\n","metadata":{}},{"cell_type":"markdown","source":"### Shape of the Data","metadata":{}},{"cell_type":"code","source":"print(f\"The data set has {df.shape[0]} rows and {df.shape[1]} columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.179967Z","iopub.execute_input":"2025-11-13T15:03:19.180218Z","iopub.status.idle":"2025-11-13T15:03:19.189969Z","shell.execute_reply.started":"2025-11-13T15:03:19.180188Z","shell.execute_reply":"2025-11-13T15:03:19.185998Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Info of the Data","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.190422Z","iopub.execute_input":"2025-11-13T15:03:19.190666Z","iopub.status.idle":"2025-11-13T15:03:19.444944Z","shell.execute_reply.started":"2025-11-13T15:03:19.190648Z","shell.execute_reply":"2025-11-13T15:03:19.443895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Descriptive Statistics","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.446018Z","iopub.execute_input":"2025-11-13T15:03:19.44629Z","iopub.status.idle":"2025-11-13T15:03:19.553356Z","shell.execute_reply.started":"2025-11-13T15:03:19.446264Z","shell.execute_reply":"2025-11-13T15:03:19.552583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"# Number of Missing Values\ndf.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.554256Z","iopub.execute_input":"2025-11-13T15:03:19.554534Z","iopub.status.idle":"2025-11-13T15:03:19.699518Z","shell.execute_reply.started":"2025-11-13T15:03:19.554507Z","shell.execute_reply":"2025-11-13T15:03:19.698601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Percentage of Missing Values\n(df.isnull().sum()/len(df)*100).sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.70039Z","iopub.execute_input":"2025-11-13T15:03:19.700678Z","iopub.status.idle":"2025-11-13T15:03:19.848602Z","shell.execute_reply.started":"2025-11-13T15:03:19.700651Z","shell.execute_reply":"2025-11-13T15:03:19.847721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ” Observations\n\n- The dataset contains **148,670 rows** and **34 columns**, making it quite comprehensive for loan analysis.  \n- It includes both **numerical (13)** and **categorical (21)** features covering aspects like loan amount, property value, income, credit score, and applicant information.  \n- The **target variable** is `Status`, which indicates whether a loan has defaulted (1) or not (0).  \n- Major missing values are present in financial-related columns such as:  \n  - `Upfront_charges` (~26.7%)  \n  - `Interest_rate_spread` (~24.6%)  \n  - `rate_of_interest` (~24.5%)  \n  - `dtir1` (~16.2%)  \n  - `LTV` and `property_value` (~10.1%)  \n  - `income` (~6.1%)  \n- Minor missing values appear in categorical fields like `loan_limit`, `approv_in_adv`, `submission_of_application`, and `age`.  \n- Overall, **missingness is focused on financial attributes**, which could affect credit risk modeling if not handled properly.  \n- The dataset is rich and detailed â€” offering great potential for uncovering **patterns in borrower behavior** and **predicting loan default risk**.\n\nðŸª„ *Next, weâ€™ll move to Step 2 â€” â€œClues from the Data Crime Scene,â€ where weâ€™ll visualize and explore the patterns hidden in these features!*\n","metadata":{}},{"cell_type":"markdown","source":"# **2. Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"markdown","source":"### ðŸ•µï¸â€â™‚ï¸ Step 2: Clues from the Data Crime Scene â€” Exploratory Data Analysis (EDA)\n\nWelcome to the **data detective zone**! ðŸ”Ž  \nEvery column in our dataset hides a clue â€” about who defaults, who repays on time, and why.  \nBefore jumping to modeling, weâ€™ll visualize patterns, spot anomalies, and explore the relationships between features.  \n\nWeâ€™ll break down our investigation into three parts:\n\n1. **Univariate Exploration** â€” Examine each feature individually (distributions, outliers, etc.)  \n2. **Bivariate Exploration** â€” Compare features with the target variable `Status` to find what drives loan defaults.  \n3. **Multivariate Insights** â€” Understand how multiple factors combine to affect risk.\n\nLetâ€™s start with **Univariate Analysis** â€” the individual suspects of our data story!\n","metadata":{}},{"cell_type":"markdown","source":"## **ðŸŽ¯ Step 2.1: Univariate Analysis â€” Meet the Main Characters**\n\nBefore solving our loan-default mystery, letâ€™s get to know our key players (features).  \nWho are they? How do they behave? Any unusual suspects?\n\nWeâ€™ll start by exploring **numeric** and **categorical** features individually  \nto uncover patterns, distributions, and possible outliers.\n","metadata":{}},{"cell_type":"code","source":"# Separate numeric and categorical columns\nnumeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\ncategorical_cols = df.select_dtypes(include=['object', 'category']).columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.851613Z","iopub.execute_input":"2025-11-13T15:03:19.851888Z","iopub.status.idle":"2025-11-13T15:03:19.875726Z","shell.execute_reply.started":"2025-11-13T15:03:19.851865Z","shell.execute_reply":"2025-11-13T15:03:19.874985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(numeric_cols) + len(categorical_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.876524Z","iopub.execute_input":"2025-11-13T15:03:19.876729Z","iopub.status.idle":"2025-11-13T15:03:19.882142Z","shell.execute_reply.started":"2025-11-13T15:03:19.876713Z","shell.execute_reply":"2025-11-13T15:03:19.881267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Numeric Columns","metadata":{}},{"cell_type":"code","source":"plt.style.use('seaborn-v0_8-whitegrid')\nfor col in numeric_cols:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[col], kde=True, bins=40, color='#FDBB9F')  \n    plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:19.883159Z","iopub.execute_input":"2025-11-13T15:03:19.883456Z","iopub.status.idle":"2025-11-13T15:03:29.320504Z","shell.execute_reply.started":"2025-11-13T15:03:19.883431Z","shell.execute_reply":"2025-11-13T15:03:29.319674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate and print skewness for each numeric column\nfor col in numeric_cols:\n    skewness = df[col].skew()\n    print(f\"The Skewness of {col} is {skewness:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:29.321516Z","iopub.execute_input":"2025-11-13T15:03:29.321744Z","iopub.status.idle":"2025-11-13T15:03:29.352887Z","shell.execute_reply.started":"2025-11-13T15:03:29.321727Z","shell.execute_reply":"2025-11-13T15:03:29.35222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“ˆ Skewness Analysis â€” Detecting Data Imbalance in Disguise\n\nBefore we move deeper, letâ€™s check how â€œbalancedâ€ or â€œtiltedâ€ our numeric features are.  \nSkewness helps us understand whether most data points lie on one side of the mean.  \nA skewness of **0** means symmetry, while values above **1** or below **â€“1** indicate strong skew.\n\n| Feature | Skewness | Interpretation |\n|----------|-----------|----------------|\n| `ID` | 0.00 | Uniformly distributed â€” purely an identifier. |\n| `year` | 0.00 | Constant across all entries (only 2019). |\n| `loan_amount` | 1.67 | **Right-skewed** â€” many small loans, few very large ones. |\n| `rate_of_interest` | 0.39 | Slightly **right-skewed** â€” mostly centered near average rates. |\n| `Interest_rate_spread` | 0.28 | Fairly **symmetric**, minimal distortion. |\n| `Upfront_charges` | 1.75 | **Highly right-skewed** â€” most borrowers pay small fees, some pay huge amounts. |\n| `term` | -2.17 | **Left-skewed** â€” most loans are long-term, few are short-term. |\n| `property_value` | 4.59 | **Extremely right-skewed** â€” a few high-value properties stretch the scale. |\n| `income` | 17.31 | **Severely right-skewed** â€” large income disparity among borrowers. |\n| `Credit_Score` | 0.00 | **Balanced** â€” most borrowers have mid-range scores. |\n| `LTV` | 120.62 | **Extremely right-skewed** â€” possibly contains outliers or errors. |\n| `Status` | 1.18 | **Right-skewed** â€” far fewer defaults than successful loans. |\n| `dtir1` | -0.55 | Slight **left skew**, fairly stable distribution. |\n\n---\n\n### ðŸ§© Observations\n\n- Most **financial variables** (`loan_amount`, `income`, `Upfront_charges`, `property_value`, `LTV`) show **strong right skewness**, indicating a few extreme high-value cases.  \n- **LTV** and **income** have *very high skewness* â€” they may need **log transformation** to stabilize model performance later.  \n- **Term** and **dtir1** are slightly **left-skewed**, meaning most loans have longer durations and moderate debt ratios.  \n- **Rate-related features** (`rate_of_interest`, `Interest_rate_spread`) are nicely balanced â€” good signs for numerical stability.  \n- The **target `Status`** is slightly right-skewed, confirming **class imbalance** (more non-defaults than defaults).  \n\nðŸ’¡ *Insight:*  \nOur data leans heavily on the right side â€” just like income distribution in real life!  \nWeâ€™ll handle this skewness in preprocessing to prevent models from getting biased toward common low-value ranges.\n","metadata":{}},{"cell_type":"markdown","source":"### Categorical Columns","metadata":{}},{"cell_type":"code","source":"for col in categorical_cols:\n    print(f\"\\nValue counts for '{col}':\")\n    print(df[col].value_counts())\n    print('-' * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:29.353659Z","iopub.execute_input":"2025-11-13T15:03:29.354044Z","iopub.status.idle":"2025-11-13T15:03:29.570381Z","shell.execute_reply.started":"2025-11-13T15:03:29.35402Z","shell.execute_reply":"2025-11-13T15:03:29.569656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Count Plots for Categorical Features\nplt.style.use('seaborn-v0_8-whitegrid')\n\nfor col in categorical_cols:\n    plt.figure(figsize=(10, 6))\n    order = df[col].value_counts().index[:10]\n    sns.countplot(x=df[col],order=order, palette='coolwarm')\n    plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:29.571183Z","iopub.execute_input":"2025-11-13T15:03:29.571374Z","iopub.status.idle":"2025-11-13T15:03:33.397393Z","shell.execute_reply.started":"2025-11-13T15:03:29.571359Z","shell.execute_reply":"2025-11-13T15:03:33.396501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ§© Categorical Feature Observations\n\n- **loan_limit**: Most loans fall under cf (Conforming), showing that standardized loan types dominate.\n- **Gender**: Majority are Male and Joint applicants; Sex Not Available appears quite frequently (possible missing or anonymized data).\n- **approv_in_adv**: Most applications are not pre-approved.\n- **loan_type**: Type1 loans are most common, followed by Type2 and Type3.\n- **loan_purpose**: Purposes p3 and p4 dominate over p1 and p2.\n- **Credit_Worthiness**: Majority are l1 (good credit).\n- **open_credit**: Almost all have no open credit (class imbalance).\n- **business_or_commercial**: Most loans are not business/commercial related.\n- **Neg_ammortization**: not_neg category dominates (rarely used negative amortization).\n- **interest_only**: not_int (no interest-only loans) are the majority.\n- **lump_sum_payment**: not_lpsm (no lump sum payments) dominate.\n- **construction_type**: Mostly site-built (sb) homes.\n- **occupancy_type**: Majority are primary residence (pr).\n- **Secured_by**: Nearly all loans are secured by home.\n- **total_units**: Mostly 1-unit properties.\n- **credit_type**: CIB and CRIF are top credit sources.\n- **co-applicant_credit_type**: Even split between CIB and EXP.\n- **age**: Most applicants are in the 45â€“54 age group; very few under 25.\n- **submission_of_application**: Majority submitted to the institution directly.\n- **Region**: North and South regions dominate applications.\n- **Security_Type**: Almost all are direct security type.","metadata":{}},{"cell_type":"markdown","source":"## ðŸŽ¯ Step 2.2: Bivariate Analysis â€” Connecting the Dots\n\nNow that weâ€™ve met our datasetâ€™s main characters, itâ€™s time to see how they interact! ðŸ’¬\nDo certain patterns in income, credit score, or loan purpose lead to loan approval or rejection?\nAre there relationships hiding beneath the surface that could explain financial behaviors?\n\nIn this step, weâ€™ll explore how two variables relate â€”\nfocusing especially on numeric vs target and categorical vs target relationships\nto uncover meaningful insights and hidden dependencies.","metadata":{}},{"cell_type":"code","source":"for i in numeric_cols:\n    grouped_data = df.groupby('Status')[i].describe()\n    print(f\"Summary for {i}:\")\n    print(grouped_data)\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:33.398218Z","iopub.execute_input":"2025-11-13T15:03:33.398519Z","iopub.status.idle":"2025-11-13T15:03:33.579769Z","shell.execute_reply.started":"2025-11-13T15:03:33.39849Z","shell.execute_reply":"2025-11-13T15:03:33.579099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in numeric_cols:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Status', y=col, data=df, palette='cool')\n    plt.title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n    plt.xlabel('Status')\n    plt.ylabel(col)\n    plt.xticks(rotation=0)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:33.580494Z","iopub.execute_input":"2025-11-13T15:03:33.580703Z","iopub.status.idle":"2025-11-13T15:03:35.557841Z","shell.execute_reply.started":"2025-11-13T15:03:33.580685Z","shell.execute_reply":"2025-11-13T15:03:35.55712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_cols:\n    grouped_data = df.groupby('Status')[col].value_counts().unstack(fill_value=0)\n    print(f\"Counts for {col} grouped by Status:\")\n    print(grouped_data)\n    print(\"-\" * 50)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:35.558577Z","iopub.execute_input":"2025-11-13T15:03:35.558794Z","iopub.status.idle":"2025-11-13T15:03:35.843951Z","shell.execute_reply.started":"2025-11-13T15:03:35.558777Z","shell.execute_reply":"2025-11-13T15:03:35.843255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_cols:\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=df, x=col, hue='Status', palette='coolwarm')\n    plt.title(f'Distribution of {col} by Status', fontsize=14, fontweight='bold')\n    plt.xlabel(col)\n    plt.ylabel('Count')\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.legend(title='Status')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:35.844665Z","iopub.execute_input":"2025-11-13T15:03:35.844922Z","iopub.status.idle":"2025-11-13T15:03:40.922998Z","shell.execute_reply.started":"2025-11-13T15:03:35.844903Z","shell.execute_reply":"2025-11-13T15:03:40.922021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ“Š **Observations from Bivariate Analysis**\n\n#### ðŸ”¹ Numeric Features vs Loan Status\n\n- **Loan Amount:** Non-defaulters have a slightly **higher average loan amount (~335K)** than defaulters (~319K), but the difference isnâ€™t huge â€” indicating loan size alone doesnâ€™t drive default strongly.  \n- **Rate of Interest:** The **average interest rate for defaulters (4.35%)** is higher than for non-defaulters (4.04%), suggesting that riskier borrowers are charged more interest â€” and are more likely to default.  \n- **Interest Rate Spread:** Data is mostly missing for defaulters, making it hard to compare, but higher spreads typically indicate riskier loans.  \n- **Upfront Charges:** Defaulters pay **lower upfront charges (avg â‰ˆ 1.5K)** compared to non-defaulters (â‰ˆ 3.2K), possibly due to smaller or subsidized loans.  \n- **Property Value:** Defaulters own **less valuable properties (avg â‰ˆ 457K)** than non-defaulters (â‰ˆ 505K), hinting at weaker financial backgrounds.  \n- **Income:** Defaulters earn less (â‰ˆ 6.2K) than non-defaulters (â‰ˆ 7.2K), which directly correlates with repayment capacity.  \n- **Credit Score:** Interestingly, both groups have similar average credit scores (~700), but variability might hide riskier subgroups.  \n- **Loan-to-Value (LTV):** Defaulters have slightly **higher LTV ratios (76.3%)** compared to non-defaulters (72.0%), meaning they borrow more relative to property value â€” a risk factor for default.  \n- **DTIR (Debt-to-Income Ratio):** Defaulters have a **higher average DTIR (â‰ˆ 39.6)** than non-defaulters (â‰ˆ 37.3), showing that more indebted borrowers tend to default.\n\n---\n\n#### ðŸ”¹ Categorical Features vs Loan Status\n\n- **Loan Limit:** Majority are **conforming (cf)** loans, with slightly fewer defaults â€” non-conforming loans (ncf) show higher risk.  \n- **Gender:** **Male** and **Joint** applicants dominate, but **Sex Not Available** category has a surprisingly high proportion of defaults â€” possible data quality or demographic pattern.  \n- **Approval in Advance:** Borrowers with **pre-approved loans (pre)** default less â€” early screening clearly helps reduce risk.  \n- **Loan Type:** **Type1** loans (possibly standard loans) perform better, while **Type2/Type3** loans default more often.  \n- **Loan Purpose:** **p3 and p4** (possibly refinancing or non-primary purposes) have more defaults than **p1 (primary)** loans.  \n- **Credit Worthiness:** As expected, borrowers marked as **â€œl2â€ (lower credit quality)** default at much higher rates.  \n- **Open Credit:** Those without open credit accounts (**nopc**) default more, indicating a lack of active credit history.  \n- **Business or Commercial Loans:** Business loans (**b/c**) have a significantly higher default rate than personal ones (**nob/c**).  \n- **Negative Amortization:** Loans with **negative amortization (neg_amm)** show higher defaults â€” deferred interest is risky.  \n- **Interest Only Loans:** Default rate is higher for **interest-only** loans compared to standard ones.  \n- **Lump Sum Payment:** Loans with **lump-sum payments (lpsm)** have more defaults â€” large end payments increase pressure.  \n- **Construction Type:** Very few **manufactured homes (mh)** exist, but all of them defaulted â€” a clear risk signal.  \n- **Occupancy Type:** **Investor (ir)** and **secondary (sr)** properties default more than **primary residences (pr)**.  \n- **Secured By:** **Land-secured loans** are few but all defaulted â€” less liquid security increases risk.  \n- **Total Units:** Multi-unit properties (**3U, 4U**) have slightly higher default rates compared to single-unit (**1U**) properties.  \n- **Credit Type:** Defaulters are more common among **EQUI** and **CIB** types â€” possibly lower credit bureau grades.  \n- **Co-applicant Credit Type:** Defaults are higher when the **co-applicant uses EXP** credit â€” a minor but consistent pattern.  \n- **Age:** Younger borrowers (**25â€“34**) and elderly (**>74**) show higher default rates â€” financial instability on both ends.  \n- **Submission of Application:** **Direct submissions (to_inst)** tend to perform better than **indirect (not_inst)** applications.  \n- **Region:** **South** and **North** regions dominate both volume and defaults, suggesting possible regional economic differences.  \n- **Security Type:** All **indirectly secured loans** defaulted â€” direct security offers better repayment confidence.\n\n---\n\n### ðŸ’¡ **Key Insights**\n\n- Defaulters tend to have **lower income, lower property value, higher interest rate, and higher LTV** â€” all pointing toward higher financial risk.  \n- Riskier loan structures (**interest-only, lump-sum, or business loans**) are more likely to default.  \n- Pre-approval, better creditworthiness, and primary residence ownership **significantly reduce default probability**.  \n- Regional and demographic patterns suggest **socio-economic influences** in repayment behavior.","metadata":{}},{"cell_type":"markdown","source":"## ðŸ”º **Step 2.3: Multivariate Analysis â€” When the Variables Collide!**\n\nNow that weâ€™ve explored each feature individually (Univariate) and in pairs (Bivariate),  \nitâ€™s time to uncover how **multiple features interact together** to influence loan defaults.  \n\nReal-world data is rarely one-dimensional â€” borrowersâ€™ income, credit score, and loan value  \noften **combine in complex ways** to shape repayment behavior.  \n\nIn this step, weâ€™ll dive into:\n- ðŸ“ˆ **Pairplots and correlation patterns** among key numeric variables  \n- ðŸŽ¯ **Multivariate boxplots and heatmaps** to detect hidden relationships  \n- ðŸ§© **Feature interactions** that may drive default risk  \n\nLetâ€™s see what happens when all the key players share the stage!\n","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:40.923658Z","iopub.execute_input":"2025-11-13T15:03:40.923902Z","iopub.status.idle":"2025-11-13T15:03:41.077439Z","shell.execute_reply.started":"2025-11-13T15:03:40.923884Z","shell.execute_reply":"2025-11-13T15:03:41.07683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.pairplot(df, vars=['loan_amount', 'Credit_Score', 'Status'], hue='Status', palette='coolwarm')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:03:41.07822Z","iopub.execute_input":"2025-11-13T15:03:41.078793Z","iopub.status.idle":"2025-11-13T15:04:25.603957Z","shell.execute_reply.started":"2025-11-13T15:03:41.078765Z","shell.execute_reply":"2025-11-13T15:04:25.603285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in categorical_cols:\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x=col, y='loan_amount', hue='Status', data=df, palette='coolwarm')\n    plt.title(f'Loan Amount by {col} and Status',fontsize=16, fontweight='bold')\n    plt.xlabel(col)\n    plt.ylabel('Loan Amount')\n    plt.xticks(rotation=0)\n    plt.legend(title='Status')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:04:25.604884Z","iopub.execute_input":"2025-11-13T15:04:25.605423Z","iopub.status.idle":"2025-11-13T15:04:56.87997Z","shell.execute_reply.started":"2025-11-13T15:04:25.605394Z","shell.execute_reply":"2025-11-13T15:04:56.879184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n\nplt.figure(figsize=(14, 10))\nsns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Correlation Matrix of Numerical Features\", fontsize=16, fontweight='bold')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:04:56.880741Z","iopub.execute_input":"2025-11-13T15:04:56.881008Z","iopub.status.idle":"2025-11-13T15:04:57.461723Z","shell.execute_reply.started":"2025-11-13T15:04:56.88099Z","shell.execute_reply":"2025-11-13T15:04:57.460999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ðŸ§© Correlation Matrix â€” Insights & Observations\n\n- **Loan Amount â†” Property Value (0.73)** â†’ Very strong positive correlation, meaning larger properties tend to have higher loan amounts.  \n- **Loan Amount â†” Income (0.46)** â†’ Moderate positive correlation, suggesting that applicants with higher income usually take larger loans.  \n- **Loan Amount â†” Term (0.17)** â†’ Slight positive correlation â€” higher loan amounts might slightly increase loan duration.  \n- **Loan Amount â†” Interest Rate (-0.15)** â†’ Weak negative correlation â€” larger loans tend to have slightly lower interest rates, possibly due to better credit terms for high-value customers.  \n\n- **Property Value â†” Income (0.41)** â†’ Strong positive correlation â€” higher income is linked to more expensive properties.  \n- **Property Value â†” LTV (-0.21)** â†’ Negative correlation â€” when property value increases, the loan-to-value ratio decreases, indicating lower lending risk.  \n\n- **Rate of Interest â†” Interest Rate Spread (0.61)** â†’ Strong positive correlation, which makes sense since both metrics reflect lending cost structures.  \n- **Interest Rate Spread â†” Loan Amount (-0.37)** â†’ Negative correlation â€” larger loans tend to have smaller spreads, likely due to better-negotiated terms.  \n\n- **Income â†” DTI Ratio (-0.26)** â†’ Moderate negative correlation â€” higher-income applicants have lower debt-to-income ratios, implying better financial stability.  \n- **LTV â†” DTI Ratio (0.16)** â†’ Positive correlation â€” higher LTV loans are associated with slightly higher debt burdens.  \n\n- **Credit Score â†” Rate of Interest (-0.00)** â†’ Almost zero correlation â€” credit scores have minimal linear influence on interest rates in this dataset.  \n- **Credit Score â†” Loan Amount (0.00)** â†’ No significant correlation â€” loan amounts arenâ€™t directly tied to credit scores.  \n\n- **Status (Target) â†” DTI Ratio (0.08)** â†’ Weak positive correlation â€” higher DTI ratios slightly increase default likelihood.  \n- **Status â†” Loan Amount (-0.03)** and **Status â†” Property Value (-0.04)** â†’ Weak negative correlations â€” smaller loans and properties show a slightly higher risk of default.  \n\n---\n\n### ðŸ’¡ Key Takeaway\nThe dataset shows meaningful relationships between **Loan Amount, Property Value, and Income**, forming the backbone of financial decision-making.  \nHowever, **Credit Score** and **Interest Rate** appear surprisingly weakly correlated â€” indicating that other factors might play a stronger role in determining loan status.\n","metadata":{}},{"cell_type":"markdown","source":"#  **3. Data Preprocessing â€” Preparing the Ground for Modeling**\n\nBefore diving into model building, we need to ensure our data is clean, consistent,  \nand ready for machine learning algorithms to understand.  \n\nIn this step, weâ€™ll handle **missing values**, **encode categorical variables**, **scale numeric features**,  \nand **split the dataset** into training and testing sets â€” setting the stage for accurate predictions.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, average_precision_score\n\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:27:19.37108Z","iopub.execute_input":"2025-11-13T16:27:19.371695Z","iopub.status.idle":"2025-11-13T16:27:19.376049Z","shell.execute_reply.started":"2025-11-13T16:27:19.371668Z","shell.execute_reply":"2025-11-13T16:27:19.375192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_remove = [\n    'ID', 'year', 'rate_of_interest', 'Interest_rate_spread', 'Upfront_charges',\n    'business_or_commercial', 'Secured_by', 'Security_Type'\n]\ndf = df.drop(columns=columns_to_remove, errors='ignore')\n\n# Handle credit_type leakage\ndf['credit_type'] = df['credit_type'].replace('EQUI', np.nan)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:04:57.214424Z","iopub.execute_input":"2025-11-13T16:04:57.214718Z","iopub.status.idle":"2025-11-13T16:04:57.250217Z","shell.execute_reply.started":"2025-11-13T16:04:57.214693Z","shell.execute_reply":"2025-11-13T16:04:57.249593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# 4. Split Features/Target\n# ==========================\nX = df.drop('Status', axis=1)\ny = df['Status']\n\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:05:20.359132Z","iopub.execute_input":"2025-11-13T16:05:20.359721Z","iopub.status.idle":"2025-11-13T16:05:20.420995Z","shell.execute_reply.started":"2025-11-13T16:05:20.359699Z","shell.execute_reply":"2025-11-13T16:05:20.420324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# 5. Preprocessing Pipelines\n# ==========================\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),  # fills NaN including EQUI\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\n# ==========================\n# 6. Train/Test Split\n# ==========================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# ==========================\n# 7. Preprocess Data\n# ==========================\nX_train_processed = preprocessor.fit_transform(X_train)\nX_train_processed = np.nan_to_num(X_train_processed).astype(np.float32)\n\nX_test_processed = preprocessor.transform(X_test)\nX_test_processed = np.nan_to_num(X_test_processed).astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:05:53.237875Z","iopub.execute_input":"2025-11-13T16:05:53.238648Z","iopub.status.idle":"2025-11-13T16:05:54.410707Z","shell.execute_reply.started":"2025-11-13T16:05:53.23862Z","shell.execute_reply":"2025-11-13T16:05:54.409855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# 8. Handle Class Imbalance\n# ==========================\nneg = (y_train == 0).sum()\npos = (y_train == 1).sum()\nscale_pos_weight = neg / pos\nprint(f\"Scale_pos_weight: {scale_pos_weight:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:06:28.306644Z","iopub.execute_input":"2025-11-13T16:06:28.307385Z","iopub.status.idle":"2025-11-13T16:06:28.312839Z","shell.execute_reply.started":"2025-11-13T16:06:28.307359Z","shell.execute_reply":"2025-11-13T16:06:28.31222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Training the Models**","metadata":{}},{"cell_type":"markdown","source":"## **4.1 XGBOOST**","metadata":{}},{"cell_type":"code","source":"# ==========================\n# 9. Train XGBoost Model\n# ==========================\nxgb_model = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=4,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    min_child_weight=5,\n    gamma=0.2,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    scale_pos_weight=scale_pos_weight,\n    random_state=42,\n    n_jobs=-1,\n    tree_method='hist',\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\nxgb_model.fit(X_train_processed, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:06:52.802767Z","iopub.execute_input":"2025-11-13T16:06:52.803055Z","iopub.status.idle":"2025-11-13T16:06:56.032432Z","shell.execute_reply.started":"2025-11-13T16:06:52.803032Z","shell.execute_reply":"2025-11-13T16:06:56.031789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# 10. Evaluate Model\n# ==========================\ny_pred = xgb_model.predict(X_test_processed)\ny_pred_proba = xgb_model.predict_proba(X_test_processed)[:, 1]\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba))\nprint(\"Precision-Recall AUC:\", average_precision_score(y_test, y_pred_proba))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:07:11.209347Z","iopub.execute_input":"2025-11-13T16:07:11.209943Z","iopub.status.idle":"2025-11-13T16:07:11.413665Z","shell.execute_reply.started":"2025-11-13T16:07:11.20992Z","shell.execute_reply":"2025-11-13T16:07:11.412918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.2 Gradient Boost**","metadata":{}},{"cell_type":"code","source":"neg = (y_train == 0).sum()\npos = (y_train == 1).sum()\nweight_for_0 = 1\nweight_for_1 = neg / pos\nsample_weights = y_train.apply(lambda x: weight_for_1 if x==1 else weight_for_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:18:50.868033Z","iopub.execute_input":"2025-11-13T16:18:50.868287Z","iopub.status.idle":"2025-11-13T16:18:50.908723Z","shell.execute_reply.started":"2025-11-13T16:18:50.86827Z","shell.execute_reply":"2025-11-13T16:18:50.908158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gb_model = GradientBoostingClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=4,\n    subsample=0.8,\n    max_features=0.8,\n    random_state=42\n)\n\ngb_model.fit(X_train_processed, y_train, sample_weight=sample_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:18:53.097754Z","iopub.execute_input":"2025-11-13T16:18:53.098413Z","iopub.status.idle":"2025-11-13T16:20:54.787662Z","shell.execute_reply.started":"2025-11-13T16:18:53.098387Z","shell.execute_reply":"2025-11-13T16:20:54.786837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred_gb = gb_model.predict(X_test_processed)\ny_pred_proba_gb = gb_model.predict_proba(X_test_processed)[:,1]\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_gb))\nprint(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba_gb))\nprint(\"Precision-Recall AUC:\", average_precision_score(y_test, y_pred_proba_gb))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:21:00.165263Z","iopub.execute_input":"2025-11-13T16:21:00.16576Z","iopub.status.idle":"2025-11-13T16:21:00.740737Z","shell.execute_reply.started":"2025-11-13T16:21:00.165734Z","shell.execute_reply":"2025-11-13T16:21:00.740058Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.3 Logistic Regression**","metadata":{}},{"cell_type":"code","source":"log_model = LogisticRegression(\n    max_iter=1000,\n    class_weight='balanced',\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:28:13.650396Z","iopub.execute_input":"2025-11-13T16:28:13.65067Z","iopub.status.idle":"2025-11-13T16:28:13.655503Z","shell.execute_reply.started":"2025-11-13T16:28:13.65065Z","shell.execute_reply":"2025-11-13T16:28:13.654591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_model.fit(X_train_processed, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:28:41.816911Z","iopub.execute_input":"2025-11-13T16:28:41.817456Z","iopub.status.idle":"2025-11-13T16:28:43.928762Z","shell.execute_reply.started":"2025-11-13T16:28:41.817431Z","shell.execute_reply":"2025-11-13T16:28:43.927316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================\n# 3. Predict\n# ==========================\ny_pred_log = log_model.predict(X_test_processed)\ny_pred_proba_log = log_model.predict_proba(X_test_processed)[:,1]\n\n# ==========================\n# 4. Evaluate\n# ==========================\nprint(\"=== Logistic Regression ===\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_log))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_log))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_log))\nprint(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba_log))\nprint(\"Precision-Recall AUC:\", average_precision_score(y_test, y_pred_proba_log))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:29:01.505947Z","iopub.execute_input":"2025-11-13T16:29:01.506513Z","iopub.status.idle":"2025-11-13T16:29:01.548918Z","shell.execute_reply.started":"2025-11-13T16:29:01.506492Z","shell.execute_reply":"2025-11-13T16:29:01.54812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4.4 Comperison**","metadata":{}},{"cell_type":"code","source":"def plot_three_models(y_true, y_pred_xgb, y_proba_xgb, \n                      y_pred_gb, y_proba_gb, \n                      y_pred_log, y_proba_log):\n    \n    # ---------- Confusion Matrices ----------\n    fig, axes = plt.subplots(1,3, figsize=(18,5))\n    \n    sns.heatmap(confusion_matrix(y_true, y_pred_xgb), annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Non-Default','Default'], yticklabels=['Non-Default','Default'], ax=axes[0])\n    axes[0].set_title('XGBoost Confusion Matrix'); axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n\n    sns.heatmap(confusion_matrix(y_true, y_pred_gb), annot=True, fmt='d', cmap='Greens',\n                xticklabels=['Non-Default','Default'], yticklabels=['Non-Default','Default'], ax=axes[1])\n    axes[1].set_title('Gradient Boosting Confusion Matrix'); axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n\n    sns.heatmap(confusion_matrix(y_true, y_pred_log), annot=True, fmt='d', cmap='Oranges',\n                xticklabels=['Non-Default','Default'], yticklabels=['Non-Default','Default'], ax=axes[2])\n    axes[2].set_title('Logistic Regression Confusion Matrix'); axes[2].set_xlabel('Predicted'); axes[2].set_ylabel('Actual')\n    \n    plt.show()\n\n    # ---------- ROC Curves ----------\n    fpr_xgb, tpr_xgb, _ = roc_curve(y_true, y_proba_xgb)\n    fpr_gb, tpr_gb, _ = roc_curve(y_true, y_proba_gb)\n    fpr_log, tpr_log, _ = roc_curve(y_true, y_proba_log)\n    \n    plt.figure(figsize=(7,6))\n    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost ROC AUC = {auc(fpr_xgb, tpr_xgb):.3f}', color='blue')\n    plt.plot(fpr_gb, tpr_gb, label=f'GB ROC AUC = {auc(fpr_gb, tpr_gb):.3f}', color='green')\n    plt.plot(fpr_log, tpr_log, label=f'LogReg ROC AUC = {auc(fpr_log, tpr_log):.3f}', color='orange')\n    plt.plot([0,1],[0,1], 'k--')\n    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve Comparison')\n    plt.legend(); plt.show()\n\n    # ---------- Precision-Recall Curves ----------\n    precision_xgb, recall_xgb, _ = precision_recall_curve(y_true, y_proba_xgb)\n    precision_gb, recall_gb, _ = precision_recall_curve(y_true, y_proba_gb)\n    precision_log, recall_log, _ = precision_recall_curve(y_true, y_proba_log)\n    \n    plt.figure(figsize=(7,6))\n    plt.plot(recall_xgb, precision_xgb, label=f'XGBoost PR AUC = {average_precision_score(y_true, y_proba_xgb):.3f}', color='blue')\n    plt.plot(recall_gb, precision_gb, label=f'GB PR AUC = {average_precision_score(y_true, y_proba_gb):.3f}', color='green')\n    plt.plot(recall_log, precision_log, label=f'LogReg PR AUC = {average_precision_score(y_true, y_proba_log):.3f}', color='orange')\n    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve Comparison')\n    plt.legend(); plt.show()\n\n\n# ==========================\n# Run the comparison\n# ==========================\nplot_three_models(\n    y_test, \n    y_pred, y_pred_proba,           # XGBoost\n    y_pred_gb, y_pred_proba_gb,     # Gradient Boosting\n    y_pred_log, y_pred_proba_log     # Logistic Regression\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T16:30:45.827979Z","iopub.execute_input":"2025-11-13T16:30:45.828751Z","iopub.status.idle":"2025-11-13T16:30:47.108094Z","shell.execute_reply.started":"2025-11-13T16:30:45.828724Z","shell.execute_reply":"2025-11-13T16:30:47.107176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}